{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A2wZcY7FTZSj"
   },
   "source": [
    "Code below defines a GP network and supporting functions based on code found [here](https://github.com/RaulAstudillo06/BOFN/blob/main/bofn/models/gp_network.py)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "NCrymI9zTB8P"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from __future__ import annotations\n",
    "import torch\n",
    "from typing import Any, Tuple\n",
    "from botorch.models.model import Model\n",
    "from botorch.models import FixedNoiseGP\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.posteriors import Posterior\n",
    "from botorch.models.transforms import Standardize\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from torch import Tensor\n",
    "from typing import List, Optional\n",
    "\n",
    "class GaussianProcessNetwork(Model):\n",
    "    r\"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, train_X, train_Y, dag, active_input_indices, train_Yvar=None, node_GPs=None, normalization_constant_lower=None, normalization_constant_upper=None) -> None:\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "        self.train_X = train_X\n",
    "        self.train_Y = train_Y\n",
    "        self.dag = dag\n",
    "        self.n_nodes = dag.get_n_nodes()\n",
    "        self.root_nodes = dag.get_root_nodes()\n",
    "        self.active_input_indices = active_input_indices\n",
    "        self.train_Yvar = train_Yvar\n",
    "        self.noise_var = 1e-5\n",
    "\n",
    "        if node_GPs is not None:\n",
    "            self.node_GPs = node_GPs\n",
    "            self.normalization_constant_lower = normalization_constant_lower\n",
    "            self.normalization_constant_upper = normalization_constant_upper\n",
    "        else:\n",
    "            self.node_GPs = [None for k in range(self.n_nodes)]\n",
    "            self.node_mlls = [None for k in range(self.n_nodes)]\n",
    "            self.normalization_constant_lower = [[None for j in range(len(self.dag.get_parent_nodes(k)))] for k in range(self.n_nodes)]\n",
    "            self.normalization_constant_upper = [[None for j in range(len(self.dag.get_parent_nodes(k)))] for k in range(self.n_nodes)]\n",
    "\n",
    "            for k in self.root_nodes:\n",
    "                if self.active_input_indices is not None:\n",
    "                    train_X_node_k = train_X[..., self.active_input_indices[k]]\n",
    "                else:\n",
    "                    train_X_node_k = train_X\n",
    "                train_Y_node_k = train_Y[..., [k]]\n",
    "                #self.node_GPs[k] = SingleTaskGP(train_X=train_X_node_k, train_Y=train_Y_node_k, outcome_transform=Standardize(m=1, batch_shape=torch.Size([1])))\n",
    "                self.node_GPs[k] = FixedNoiseGP(train_X=train_X_node_k, train_Y=train_Y_node_k, train_Yvar=torch.ones(train_Y_node_k.shape) * self.noise_var, outcome_transform=Standardize(m=1))\n",
    "                self.node_mlls[k] = ExactMarginalLogLikelihood(self.node_GPs[k].likelihood, self.node_GPs[k])\n",
    "                fit_gpytorch_model(self.node_mlls[k])\n",
    "\n",
    "            for k in range(self.n_nodes):\n",
    "                if self.node_GPs[k] is None:\n",
    "                    aux = train_Y[..., self.dag.get_parent_nodes(k)].clone()\n",
    "                    for j in range(len(self.dag.get_parent_nodes(k))):\n",
    "                        self.normalization_constant_lower[k][j] = torch.min(aux[..., j])\n",
    "                        self.normalization_constant_upper[k][j] = torch.max(aux[..., j])\n",
    "                        aux[..., j] = (aux[..., j] - self.normalization_constant_lower[k][j])/(self.normalization_constant_upper[k][j] - self.normalization_constant_lower[k][j])\n",
    "                    train_X_node_k = torch.cat([train_X[..., self.active_input_indices[k]], aux], -1)\n",
    "                    train_Y_node_k = train_Y[..., [k]]\n",
    "                    aux_model =  FixedNoiseGP(train_X=train_X_node_k, train_Y=train_Y_node_k, train_Yvar=torch.ones(train_Y_node_k.shape) * self.noise_var, outcome_transform=Standardize(m=1))\n",
    "                    batch_shape = aux_model._aug_batch_shape\n",
    "                    #self.node_GPs[k] = SingleTaskGP(train_X=train_X_node_k, train_Y=train_Y_node_k, outcome_transform=Standardize(m=1, batch_shape=torch.Size([1])))\n",
    "                    #self.node_GPs[k] = FixedNoiseGP(train_X=train_X_node_k, train_Y=train_Y_node_k, train_Yvar=torch.ones(train_Y_node_k.shape) * 1e-6, outcome_transform=Standardize(m=1, batch_shape=torch.Size([1])))\n",
    "                    self.node_GPs[k] = FixedNoiseGP(train_X=train_X_node_k, train_Y=train_Y_node_k, train_Yvar=torch.ones(train_Y_node_k.shape) * self.noise_var, outcome_transform=Standardize(m=1, batch_shape=torch.Size([])))\n",
    "                    self.node_mlls[k] = ExactMarginalLogLikelihood(self.node_GPs[k].likelihood, self.node_GPs[k])\n",
    "                    fit_gpytorch_model(self.node_mlls[k])\n",
    "\n",
    "    def posterior(self, X: Tensor, posterior_transform=None, observation_noise=False) -> MultivariateNormalNetwork:\n",
    "        r\"\"\"Computes the posterior over model outputs at the provided points.\n",
    "        Args:\n",
    "            X: A `(batch_shape) x q x d`-dim Tensor, where `d` is the dimension\n",
    "                of the feature space and `q` is the number of points considered\n",
    "                jointly.\n",
    "            observation_noise: If True, add the observation noise from the\n",
    "                likelihood to the posterior. If a Tensor, use it directly as the\n",
    "                observation noise (must be of shape `(batch_shape) x q`).\n",
    "        Returns:\n",
    "            A `GPyTorchPosterior` object, representing a batch of `b` joint\n",
    "            distributions over `q` points. Includes observation noise if\n",
    "            specified.\n",
    "        \"\"\"\n",
    "        return MultivariateNormalNetwork(self.node_GPs, self.dag, X, self.active_input_indices, self.normalization_constant_lower, self.normalization_constant_upper)\n",
    "\n",
    "    def forward(self, x: Tensor) -> MultivariateNormalNetwork:\n",
    "        return MultivariateNormalNetwork(self.node_GPs, self.dag, x, self.active_input_indices, self.normalization_constant)\n",
    "\n",
    "    def condition_on_observations(self, X: Tensor, Y: Tensor, **kwargs: Any) -> Model:\n",
    "        r\"\"\"Condition the model on new observations.\n",
    "        Args:\n",
    "            X: A `batch_shape x n' x d`-dim Tensor, where `d` is the dimension of\n",
    "                the feature space, `n'` is the number of points per batch, and\n",
    "                `batch_shape` is the batch shape (must be compatible with the\n",
    "                batch shape of the model).\n",
    "            Y: A `batch_shape' x n' x m`-dim Tensor, where `m` is the number of\n",
    "                model outputs, `n'` is the number of points per batch, and\n",
    "                `batch_shape'` is the batch shape of the observations.\n",
    "                `batch_shape'` must be broadcastable to `batch_shape` using\n",
    "                standard broadcasting semantics. If `Y` has fewer batch dimensions\n",
    "                than `X`, it is assumed that the missing batch dimensions are\n",
    "                the same for all `Y`.\n",
    "        Returns:\n",
    "            A `Model` object of the same type, representing the original model\n",
    "            conditioned on the new observations `(X, Y)` (and possibly noise\n",
    "            observations passed in via kwargs).\n",
    "        \"\"\"\n",
    "        fantasy_models = [None for k in range(self.n_nodes)]\n",
    "\n",
    "        for k in self.root_nodes:\n",
    "            if self.active_input_indices is not None:\n",
    "                X_node_k = X[..., self.active_input_indices[k]]\n",
    "            else:\n",
    "                X_node_k = X\n",
    "            Y_node_k = Y[..., [k]]\n",
    "            fantasy_models[k] = self.node_GPs[k].condition_on_observations(X_node_k, Y_node_k, noise=torch.ones(Y_node_k.shape[1:]) * self.noise_var)\n",
    "\n",
    "        for k in range(self.n_nodes):\n",
    "            if fantasy_models[k] is None:\n",
    "                aux = Y[..., self.dag.get_parent_nodes(k)].clone()\n",
    "                for j in range(len(self.dag.get_parent_nodes(k))):\n",
    "                    aux[..., j] = (aux[..., j] - self.normalization_constant_lower[k][j])/(self.normalization_constant_upper[k][j] - self.normalization_constant_lower[k][j])\n",
    "                aux_shape = [aux.shape[0]] + [1] * X[..., self.active_input_indices[k]].ndim\n",
    "                X_aux = X[..., self.active_input_indices[k]].unsqueeze(0).repeat(*aux_shape)\n",
    "                X_node_k = torch.cat([X_aux, aux], -1)\n",
    "                Y_node_k = Y[..., [k]]\n",
    "                fantasy_models[k] = self.node_GPs[k].condition_on_observations(X_node_k, Y_node_k, noise=torch.ones(Y_node_k.shape[1:]) * self.noise_var)\n",
    "\n",
    "        return GaussianProcessNetwork(dag=self.dag, train_X=X, train_Y=Y, active_input_indices=self.active_input_indices, node_GPs=fantasy_models, normalization_constant_lower=self.normalization_constant_lower, normalization_constant_upper=self.normalization_constant_upper)\n",
    "\n",
    "\n",
    "class MultivariateNormalNetwork(Posterior):\n",
    "    def __init__(self, node_GPs, dag, X, indices_X=None, normalization_constant_lower=None, normalization_constant_upper=None):\n",
    "        self.node_GPs = node_GPs\n",
    "        self.dag = dag\n",
    "        self.n_nodes = dag.get_n_nodes()\n",
    "        self.root_nodes = dag.get_root_nodes()\n",
    "        self.X = X\n",
    "        self.active_input_indices = indices_X\n",
    "        self.normalization_constant_lower = normalization_constant_lower\n",
    "        self.normalization_constant_upper = normalization_constant_upper\n",
    "\n",
    "    @property\n",
    "    def device(self) -> torch.device:\n",
    "        r\"\"\"The torch device of the posterior.\"\"\"\n",
    "        return \"cpu\"\n",
    "\n",
    "    @property\n",
    "    def dtype(self) -> torch.dtype:\n",
    "        r\"\"\"The torch dtype of the posterior.\"\"\"\n",
    "        return torch.double\n",
    "\n",
    "    @property\n",
    "    def event_shape(self) -> torch.Size:\n",
    "        r\"\"\"The event shape (i.e. the shape of a single sample) of the posterior.\"\"\"\n",
    "        shape = list(self.X.shape)\n",
    "        shape[-1] = self.n_nodes\n",
    "        shape = torch.Size(shape)\n",
    "        return shape\n",
    "\n",
    "    @property\n",
    "    def base_sample_shape(self) -> torch.Size:\n",
    "        r\"\"\"The base shape of the base samples expected in `rsample`.\n",
    "\n",
    "        Informs the sampler to produce base samples of shape\n",
    "        `sample_shape x base_sample_shape`.\n",
    "        \"\"\"\n",
    "        shape = torch.Size(list([1,1,self.n_nodes]))\n",
    "        return shape\n",
    "\n",
    "    @property\n",
    "    def batch_range(self) -> Tuple[int, int]:\n",
    "        r\"\"\"The t-batch range.\n",
    "\n",
    "        This is used in samplers to identify the t-batch component of the\n",
    "        `base_sample_shape`. The base samples are expanded over the t-batches to\n",
    "        provide consistency in the acquisition values, i.e., to ensure that a\n",
    "        candidate produces same value regardless of its position on the t-batch.\n",
    "        \"\"\"\n",
    "        return (0, -1)\n",
    "\n",
    "    def rsample_from_base_samples(self, sample_shape: torch.Size, base_samples: Tensor) -> Tensor:\n",
    "        return self.rsample(sample_shape, base_samples)\n",
    "\n",
    "    def rsample(self, sample_shape=torch.Size(), base_samples=None):\n",
    "        #t0 =  time.time()\n",
    "        nodes_samples = torch.empty(sample_shape + self.event_shape)\n",
    "        nodes_samples = nodes_samples.double()\n",
    "        nodes_samples_available = [False for k in range(self.n_nodes)]\n",
    "        for k in self.root_nodes:\n",
    "            #t0 =  time.time()\n",
    "            if self.active_input_indices is not None:\n",
    "                X_node_k = self.X[..., self.active_input_indices[k]]\n",
    "            else:\n",
    "                X_node_k = self.X\n",
    "            multivariate_normal_at_node_k = self.node_GPs[k].posterior(X_node_k)\n",
    "            if base_samples is not None:\n",
    "                nodes_samples[..., k] = multivariate_normal_at_node_k.rsample(sample_shape, base_samples=base_samples[..., [k]])[..., 0]\n",
    "            else:\n",
    "                nodes_samples[..., k] = multivariate_normal_at_node_k.rsample(sample_shape)[..., 0]\n",
    "            nodes_samples_available[k] = True\n",
    "            #t1 = time.time()\n",
    "            #print('Part A of the code took: ' + str(t1 - t0))\n",
    "\n",
    "        while not all(nodes_samples_available):\n",
    "            for k in range(self.n_nodes):\n",
    "                parent_nodes = self.dag.get_parent_nodes(k)\n",
    "                if not nodes_samples_available[k] and all([nodes_samples_available[j] for j in parent_nodes]):\n",
    "                    #t0 =  time.time()\n",
    "                    parent_nodes_samples_normalized = nodes_samples[..., parent_nodes].clone()\n",
    "                    for j in range(len(parent_nodes)):\n",
    "                        parent_nodes_samples_normalized[..., j] = (parent_nodes_samples_normalized[..., j] - self.normalization_constant_lower[k][j])/(self.normalization_constant_upper[k][j] - self.normalization_constant_lower[k][j])\n",
    "                    X_node_k = self.X[..., self.active_input_indices[k]]\n",
    "                    aux_shape = [sample_shape[0]] + [1] * X_node_k.ndim\n",
    "                    X_node_k = X_node_k.unsqueeze(0).repeat(*aux_shape)\n",
    "                    X_node_k = torch.cat([X_node_k, parent_nodes_samples_normalized], -1)\n",
    "                    multivariate_normal_at_node_k = self.node_GPs[k].posterior(X_node_k)\n",
    "                    if base_samples is not None:\n",
    "                        #print(torch.sqrt(multivariate_normal_at_node_k.variance).shape)\n",
    "                        #print(torch.flatten(base_samples[..., k]).shape)\n",
    "                        my_aux = torch.sqrt(multivariate_normal_at_node_k.variance)\n",
    "                        #print(my_aux.ndim)\n",
    "                        if my_aux.ndim == 4:\n",
    "                            nodes_samples[...,k] = (multivariate_normal_at_node_k.mean + torch.einsum('abcd,a->abcd', torch.sqrt(multivariate_normal_at_node_k.variance), torch.flatten(base_samples[..., k])))[..., 0]\n",
    "                        elif my_aux.ndim == 5:\n",
    "                            nodes_samples[...,k] = (multivariate_normal_at_node_k.mean + torch.einsum('abcde,a->abcde', torch.sqrt(multivariate_normal_at_node_k.variance), torch.flatten(base_samples[..., k])))[..., 0]\n",
    "                        else:\n",
    "                            print(error)\n",
    "                    else:\n",
    "                        nodes_samples[..., k] = multivariate_normal_at_node_k.rsample()[0, ..., 0]\n",
    "                    nodes_samples_available[k] = True\n",
    "                    #t1 = time.time()\n",
    "                    #print('Part B of the code took: ' + str(t1 - t0))\n",
    "        #t1 = time.time()\n",
    "        #print('Taking this sample took: ' + str(t1 - t0))\n",
    "        return nodes_samples\n",
    "\n",
    "class DAG(object):\n",
    "\n",
    "    def __init__(self, parent_nodes:List[List[Optional[int]]]):\n",
    "        self.parent_nodes = parent_nodes\n",
    "        self.n_nodes = len(parent_nodes)\n",
    "        self.root_nodes = []\n",
    "        for k in range(self.n_nodes):\n",
    "            if len(parent_nodes[k]) == 0:\n",
    "                self.root_nodes.append(k)\n",
    "\n",
    "    def get_n_nodes(self):\n",
    "        return self.n_nodes\n",
    "\n",
    "    def get_parent_nodes(self, k):\n",
    "        return self.parent_nodes[k]\n",
    "\n",
    "    def get_root_nodes(self):\n",
    "        return self.root_nodes\n",
    "\n",
    "def generate_initial_design(num_samples: int, input_dim: int, seed=None):\n",
    "    # generate training data\n",
    "    if seed is not None:\n",
    "        old_state = torch.random.get_rng_state()\n",
    "        torch.manual_seed(seed)\n",
    "        X = torch.rand([num_samples, input_dim])\n",
    "        torch.random.set_rng_state(old_state)\n",
    "    else:\n",
    "        X = torch.rand([num_samples, input_dim])\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "SG_hM8tUu76E"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "from botorch.acquisition import MCAcquisitionFunction\n",
    "from botorch.acquisition.objective import MCAcquisitionObjective\n",
    "from botorch.models.model import Model\n",
    "from botorch.sampling.base import MCSampler\n",
    "from botorch.utils.transforms import concatenate_pending_points, t_batch_mode_transform\n",
    "from torch import Tensor\n",
    "from typing import Optional\n",
    "\n",
    "class PosteriorMean(MCAcquisitionFunction):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        model: Model,\n",
    "        sampler: Optional[MCSampler] = None,\n",
    "        objective: Optional[MCAcquisitionObjective] = None,\n",
    "        X_pending: Optional[Tensor] = None,\n",
    "    ) -> None:\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "        super().__init__(\n",
    "            model=model, sampler=sampler, objective=objective, X_pending=X_pending\n",
    "        )\n",
    "\n",
    "    @concatenate_pending_points\n",
    "    @t_batch_mode_transform()\n",
    "    def forward(self, X: Tensor) -> Tensor:\n",
    "        r\"\"\"\n",
    "        \"\"\"\n",
    "        posterior = self.model.posterior(X)\n",
    "        samples = self.sampler(posterior)\n",
    "        obj = self.objective(samples)\n",
    "        obj = obj.mean(dim=0)[..., 0]\n",
    "        return obj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "form",
    "id": "GPEf36a-xegp"
   },
   "outputs": [],
   "source": [
    "# @title\n",
    "import torch\n",
    "from torch import Tensor\n",
    "from botorch.acquisition.knowledge_gradient import qKnowledgeGradient\n",
    "from botorch.optim import optimize_acqf\n",
    "from botorch.optim.initializers import (\n",
    "    gen_batch_initial_conditions,\n",
    "    gen_one_shot_kg_initial_conditions,\n",
    ")\n",
    "\n",
    "def optimize_acqf_and_get_suggested_point(\n",
    "    acq_func,\n",
    "    bounds,\n",
    "    batch_size,\n",
    "    posterior_mean=None,\n",
    "    ) -> Tensor:\n",
    "    \"\"\"Optimizes the acquisition function, and returns a new candidate.\"\"\"\n",
    "    input_dim = bounds.shape[1]\n",
    "    num_restarts=10*input_dim\n",
    "    raw_samples=100*input_dim\n",
    "\n",
    "    ic_gen = (\n",
    "        gen_one_shot_kg_initial_conditions\n",
    "        if isinstance(acq_func, qKnowledgeGradient)\n",
    "        else gen_batch_initial_conditions\n",
    "    )\n",
    "    batch_initial_conditions = ic_gen(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=batch_size,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        options={\"batch_limit\": num_restarts},\n",
    "    )\n",
    "\n",
    "    if posterior_mean is not None:\n",
    "        baseline_candidate, _ = optimize_acqf(\n",
    "            acq_function=posterior_mean,\n",
    "            bounds=bounds,\n",
    "            q=batch_size,\n",
    "            num_restarts=num_restarts,\n",
    "            raw_samples=raw_samples,\n",
    "            options={\"batch_limit\": 5},\n",
    "        )\n",
    "\n",
    "        if isinstance(acq_func, qKnowledgeGradient):\n",
    "            augmented_q_batch_size = acq_func.get_augmented_q_batch_size(batch_size)\n",
    "            baseline_candidate = baseline_candidate.detach().repeat(1, augmented_q_batch_size, 1)\n",
    "        else:\n",
    "            baseline_candidate = baseline_candidate.detach().view(torch.Size([1, batch_size, input_dim]))\n",
    "\n",
    "        batch_initial_conditions = torch.cat([batch_initial_conditions, baseline_candidate], 0)\n",
    "        num_restarts += 1\n",
    "\n",
    "    candidate, acq_value = optimize_acqf(\n",
    "        acq_function=acq_func,\n",
    "        bounds=bounds,\n",
    "        q=batch_size,\n",
    "        num_restarts=num_restarts,\n",
    "        raw_samples=raw_samples,\n",
    "        batch_initial_conditions=batch_initial_conditions,\n",
    "        options={\"batch_limit\": 2},\n",
    "        #options={'disp': True, 'iprint': 101},\n",
    "    )\n",
    "\n",
    "    if baseline_candidate is not None:\n",
    "        baseline_acq_value = acq_func.forward(baseline_candidate)[0].detach()\n",
    "        print('Test begins')\n",
    "        print(f\"Best acquisition value: {acq_value}\")\n",
    "        print(f\"Base acquisition value: {baseline_acq_value}\")\n",
    "        print('Test ends')\n",
    "        if baseline_acq_value >= acq_value:\n",
    "            print('Baseline candidate was best found.')\n",
    "            candidate = baseline_candidate\n",
    "\n",
    "    new_x = candidate.detach().view([batch_size, input_dim])\n",
    "    return new_x\n",
    "\n",
    "def get_new_suggested_point(\n",
    "    algo: str,\n",
    "    X: Tensor,\n",
    "    network_output_at_X: Tensor,\n",
    "    objective_at_X: Tensor,\n",
    "    network_to_objective_transform: Callable,\n",
    "    dag: DAG,\n",
    "    active_input_indices: List[int],\n",
    ") -> Tensor:\n",
    "    input_dim = X.shape[-1]\n",
    "\n",
    "    if algo == \"Random\":\n",
    "        return torch.rand([1, input_dim])\n",
    "    elif algo == \"EIFN\":\n",
    "        # Model\n",
    "        model = GaussianProcessNetwork(train_X=X, train_Y=network_output_at_X, dag=dag,\n",
    "                          active_input_indices=active_input_indices)\n",
    "        # Sampler\n",
    "        qmc_sampler = SobolQMCNormalSampler(torch.Size([128]))\n",
    "        # Acquisition function\n",
    "        acquisition_function = qExpectedImprovement(\n",
    "            model=model,\n",
    "            best_f=objective_at_X.max().item(),\n",
    "            sampler=qmc_sampler,\n",
    "            objective=network_to_objective_transform,\n",
    "\n",
    "        )\n",
    "        posterior_mean_function = PosteriorMean(\n",
    "            model=model,\n",
    "            sampler=qmc_sampler,\n",
    "            objective=network_to_objective_transform,\n",
    "        )\n",
    "    elif algo == \"EICF\":\n",
    "        model = fit_gp_model(X=X, Y=network_output_at_X)\n",
    "        qmc_sampler = SobolQMCNormalSampler(num_samples=128)\n",
    "        acquisition_function = qExpectedImprovement(\n",
    "            model=model,\n",
    "            best_f=objective_at_X.max().item(),\n",
    "            sampler=qmc_sampler,\n",
    "            objective=network_to_objective_transform,\n",
    "\n",
    "        )\n",
    "        posterior_mean_function = PosteriorMean(\n",
    "            model=model,\n",
    "            sampler=qmc_sampler,\n",
    "            objective=network_to_objective_transform,\n",
    "        )\n",
    "    elif algo == \"EI\":\n",
    "        model = fit_gp_model(X=X, Y=objective_at_X)\n",
    "        acquisition_function = ExpectedImprovement(\n",
    "            model=model, best_f=objective_at_X.max().item())\n",
    "        posterior_mean_function = GPPosteriorMean(model=model)\n",
    "    elif algo == \"KG\":\n",
    "        model = fit_gp_model(X=X, Y=objective_at_X)\n",
    "        acquisition_function = qKnowledgeGradient(\n",
    "            model=model, num_fantasies=8)\n",
    "        posterior_mean_function = GPPosteriorMean(model=model)\n",
    "\n",
    "    new_x = optimize_acqf_and_get_suggested_point(\n",
    "        acq_func=acquisition_function,\n",
    "        bounds=torch.tensor([[0. for i in range(input_dim)], [\n",
    "                            1. for i in range(input_dim)]]),\n",
    "        batch_size=1,\n",
    "        posterior_mean=posterior_mean_function,\n",
    "    )\n",
    "\n",
    "    return new_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Rw5lQmCBTJyW",
    "outputId": "1325a27b-7bac-474b-8f4d-549d6d67ad13"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test begins\n",
      "Best acquisition value: 0.018715645602991826\n",
      "Base acquisition value: 0.0013770879083504395\n",
      "Test ends\n",
      "New point: tensor([[0.5490, 0.4243]])\n",
      "Took 5.683061122894287 seconds\n"
     ]
    }
   ],
   "source": [
    "# Gaussian process network example\n",
    "import torch\n",
    "from botorch.acquisition.objective import GenericMCObjective\n",
    "from botorch.settings import debug\n",
    "from torch import Tensor\n",
    "from botorch.acquisition import ExpectedImprovement, qExpectedImprovement\n",
    "from botorch.acquisition import PosteriorMean as GPPosteriorMean\n",
    "from botorch.sampling.normal import SobolQMCNormalSampler\n",
    "import time\n",
    "\n",
    "class Dropwave:\n",
    "    def __init__(self):\n",
    "        self.n_nodes = 2\n",
    "        self.input_dim = 2\n",
    "\n",
    "    def evaluate(self, X):\n",
    "        X_scaled = 10.24 * X - 5.12\n",
    "        input_shape = X_scaled.shape\n",
    "        output = torch.empty(input_shape[:-1] + torch.Size([self.n_nodes]))\n",
    "        norm_X = torch.norm(X_scaled, dim=-1)\n",
    "        output[..., 0] = norm_X\n",
    "        output[..., 1] = (1.0 + torch.cos(12.0 * norm_X)) /(2.0 + 0.5 * (norm_X ** 2))\n",
    "        return output\n",
    "\n",
    "torch.set_default_dtype(torch.float64)\n",
    "debug._set_state(True)\n",
    "\n",
    "dropwave = Dropwave()\n",
    "input_dim = dropwave.input_dim\n",
    "n_nodes = 2\n",
    "problem = 'dropwave'\n",
    "\n",
    "def function_network(X: Tensor):\n",
    "    return dropwave.evaluate(X=X)\n",
    "\n",
    "# Underlying DAG\n",
    "parent_nodes = []\n",
    "parent_nodes.append([])\n",
    "parent_nodes.append([0])\n",
    "dag= DAG(parent_nodes=parent_nodes)\n",
    "\n",
    "# Active input indices\n",
    "active_input_indices = [[0, 1], []]\n",
    "\n",
    "# Function that maps the network output to the objective value\n",
    "network_to_objective_transform = lambda Y: Y[..., -1]\n",
    "network_to_objective_transform = GenericMCObjective(network_to_objective_transform)\n",
    "\n",
    "# Generate initial data\n",
    "n_init_evals = 2*(input_dim + 1)\n",
    "trial = 42\n",
    "X = generate_initial_design(num_samples=n_init_evals, input_dim=input_dim, seed=trial)\n",
    "network_output_at_X = function_network(X)\n",
    "objective_at_X = network_to_objective_transform(network_output_at_X)\n",
    "\n",
    "# Algorithm\n",
    "algo = 'EIFN'\n",
    "\n",
    "# New suggested point\n",
    "t0 = time.time()\n",
    "new_x = get_new_suggested_point(\n",
    "    algo=algo,\n",
    "    X=X,\n",
    "    network_output_at_X=network_output_at_X,\n",
    "    objective_at_X=objective_at_X,\n",
    "    network_to_objective_transform=network_to_objective_transform,\n",
    "    dag=dag,\n",
    "    active_input_indices=active_input_indices,\n",
    ")\n",
    "t1 = time.time()\n",
    "print(f\"New point: {new_x}\")\n",
    "print(f\"Took {t1 - t0} seconds\")\n",
    "\n",
    "# # Fit the GPNetwork model\n",
    "# model = GaussianProcessNetwork(train_X=X, train_Y=network_output_at_X, dag=dag, active_input_indices=active_input_indices)\n",
    "\n",
    "# # Sampler\n",
    "# qmc_sampler = SobolQMCNormalSampler(torch.Size([128]))\n",
    "\n",
    "# # Number of test points\n",
    "# Ntest = 500\n",
    "\n",
    "# if True:\n",
    "#     # Define the mean function\n",
    "#     mean_function = PosteriorMean(model, qmc_sampler, network_to_objective_transform)\n",
    "\n",
    "#     # Calculate mean of GPnetwork over set of test points\n",
    "#     Xtest = torch.rand((Ntest,1,input_dim))\n",
    "#     objective_mean_at_Xtest = mean_function(Xtest)\n",
    "\n",
    "#     # Calculate the maximum point\n",
    "#     print(f\"Max found objective mean is: {objective_mean_at_Xtest.max()}\")\n",
    "\n",
    "# else:\n",
    "#     # Define the acquisition function\n",
    "#     acquisition_function = qExpectedImprovement(\n",
    "#         model=model,\n",
    "#         best_f=objective_at_X.max().item(),\n",
    "#         sampler=qmc_sampler,\n",
    "#         objective=network_to_objective_transform,\n",
    "#     )\n",
    "\n",
    "#     # Calculate qEI for GPnetwork over set of test points\n",
    "#     Xtest = torch.rand((Ntest,1,input_dim))\n",
    "#     acquisition_func_at_Xtest = acquisition_function(Xtest)\n",
    "\n",
    "#     # Calculate the maximum point\n",
    "#     print(f\"Max found acq func is: {acquisition_func_at_Xtest.max()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vmBg37yIEMQR"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
